# Spelling Correction with T5 and BART Transformer Models

This project implements and evaluates different Transformer-based models for spelling correction. It focuses on:
1.  Training a T5-small model from scratch on synthetically generated misspelled data.
2.  Training a BART-base model from scratch on the same synthetic data.
3.  Fine-tuning a pre-trained BART model (specifically `oliverguhr/spelling-correction-english-base`) on the synthetic data.

The project includes data preprocessing, synthetic error generation, model training, evaluation using Word Error Rate (WER), Character Error Rate (CER), and Exact-Match Accuracy, and provides examples of inference.

## Table of Contents
1.  [Project Overview](#project-overview)
2.  [Features](#features)
3.  [Models Implemented](#models-implemented)
4.  [Dataset](#dataset)
    *   [Source Data](#source-data)
    *   [Synthetic Misspelling Generation](#synthetic-misspelling-generation)
5.  [Setup and Installation](#setup-and-installation)
    *   [Prerequisites](#prerequisites)

## Project Overview
The primary goal is to build effective spelling correction systems. This is achieved by:
*   Creating a dataset of (misspelled, correct) sentence pairs by introducing random character-level errors into clean text.
*   Leveraging the power of Transformer architectures (T5 and BART) for the sequence-to-sequence task of correcting misspelled text.
*   Comparing models trained from scratch against a model that has been pre-fine-tuned for spelling correction and then further adapted to our synthetic dataset.

## Features
*   **Synthetic Data Generation**: Functions to introduce character deletions, insertions, substitutions, and transpositions to create misspelled sentences.
*   **Data Preprocessing**: Cleaning text by removing punctuation and digits, and tokenizing for T5 and BART models.
*   **T5 Model Training**: Fine-tuning `t5-small` for spelling correction.
*   **BART Model Training**:
    *   Fine-tuning `facebook/bart-base` from scratch.
    *   Further fine-tuning `oliverguhr/spelling-correction-english-base`.
*   **TensorFlow Implementation**: Models are trained and evaluated using TensorFlow and Hugging Face Transformers.
*   **Evaluation**: Performance measured using WER, CER, and Exact-Match Accuracy.
*   **Inference Examples**: Demonstrates how to use the trained models for correcting new misspelled sentences.

## Models Implemented
1.  **T5-small**: A Text-To-Text Transfer Transformer model fine-tuned from `t5-small`.
2.  **BART-base**: A Denoising Autoencoder for Pretraining Sequence-to-Sequence Models, fine-tuned from `facebook/bart-base`.
3.  **Fine-tuned BART (oliverguhr)**: The `oliverguhr/spelling-correction-english-base` model, further fine-tuned on the project's synthetic dataset.

## Dataset

### Source Data
The project assumes the availability of three TSV (Tab-Separated Values) files containing correct sentences:
*   `tune.tsv`: Sentences for the training set.
*   `validation.tsv`: Sentences for the validation set.
*   `test.tsv`: Sentences for the test set.

These files are expected to have one sentence per line in the first column.

### Synthetic Misspelling Generation
Misspelled versions of the source sentences are generated by randomly applying one of the following character-level errors to words with a certain probability:
*   **Deletion**: Removing a character.
*   **Insertion**: Adding a random lowercase character.
*   **Substitution**: Replacing a character with a random lowercase character.
*   **Transposition**: Swapping two adjacent characters.

This process creates paired datasets of (misspelled sentence, correct sentence) for training, validation, and testing.

## Setup and Installation

### Prerequisites
*   Python (3.7+ recommended)
*   pip (Python package installer)
*   (Optional but recommended for GPU) NVIDIA GPU with CUDA and cuDNN installed for TensorFlow.
