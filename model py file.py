# -*- coding: utf-8 -*-
"""20216064_20216043_20216069_20217015_20217017.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbNTazCd4ooMp8yHUSD9qpmMYKo1DDPh
"""

! pip install datasets jiwer

import os
import random
import string
import re
import numpy as np
import pandas as pd
import tensorflow as tf

from transformers import T5Tokenizer, TFT5ForConditionalGeneration
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.callbacks import EarlyStopping
from datasets import Dataset, DatasetDict
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from jiwer import wer,cer

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    T5Tokenizer,
    T5ForConditionalGeneration
)

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("'punkt' not found. Downloading...")
    nltk.download('punkt', quiet=True)
    print("'punkt' downloaded.")

print(f"TensorFlow version: {tf.__version__}")
print(f"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}")

FILES_TO_DOWNLOAD = {
    "tune": "tune.tsv",
    "validation": "validation.tsv",
    "test": "test.tsv"
}

def remove_punctuation_and_digits(text):
    chars_to_remove = string.punctuation + string.digits
    translator = str.maketrans('', '', chars_to_remove)
    return text.translate(translator)

def load_sentences_from_tsv(filepath):
    df = pd.read_csv(filepath, sep='\t', header=None, usecols=[0], names=['sentence'])
    df['sentence'] = df['sentence'].apply(lambda x: x.lstrip("' ").strip())
    df['sentence'] = df['sentence'].apply(remove_punctuation_and_digits)
    df['sentence'] = df['sentence'].apply(lambda x: " ".join(x.split()))

    return df['sentence'].tolist()

print("\nLoading sentences...")
raw_tune_sentences = load_sentences_from_tsv(os.path.join("/content/data/", FILES_TO_DOWNLOAD["tune"]))
raw_validation_sentences = load_sentences_from_tsv(os.path.join("/content/data/", FILES_TO_DOWNLOAD["validation"]))
raw_test_sentences = load_sentences_from_tsv(os.path.join("/content/data/", FILES_TO_DOWNLOAD["test"]))

print(f"Loaded {len(raw_tune_sentences)} tune sentences.")
print(f"Loaded {len(raw_validation_sentences)} validation sentences.")
print(f"Loaded {len(raw_test_sentences)} test sentences.")
print(f"Example tune sentence: {raw_tune_sentences[0]}")

def introduce_char_deletion(word):
    if len(word) <= 1:
        return word
    idx = random.randint(0, len(word) - 1)
    return word[:idx] + word[idx+1:]

def introduce_char_insertion(word):
    if not word:
        return random.choice(string.ascii_lowercase)
    idx = random.randint(0, len(word))
    char_to_insert = random.choice(string.ascii_lowercase)
    return word[:idx] + char_to_insert + word[idx:]

def introduce_char_substitution(word):
    if not word:
        return random.choice(string.ascii_lowercase)
    if len(word) == 0:
        return random.choice(string.ascii_lowercase)
    idx = random.randint(0, len(word) - 1)
    original_char = word[idx]
    new_char = random.choice(string.ascii_lowercase)
    if len(string.ascii_lowercase) > 1:
         while new_char == original_char.lower():
            new_char = random.choice(string.ascii_lowercase)
    return word[:idx] + new_char + word[idx+1:]

def introduce_char_transposition(word):
    if len(word) < 2:
        return word
    idx = random.randint(0, len(word) - 2)
    chars = list(word)
    chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
    return "".join(chars)

error_functions = [
    introduce_char_deletion,
    introduce_char_insertion,
    introduce_char_substitution,
    introduce_char_transposition
]

def generate_misspelled_sentence_random_errors(sentence, error_rate_word=0.2):
    words = sentence.split()
    misspelled_words = []

    for word in words:
        if random.random() < error_rate_word and len(word) > 2:
            temp_word = word
            num_errors_to_apply=1

            original_first_char_case = None
            if temp_word and temp_word[0].isupper():
                original_first_char_case = 'upper'
                temp_word_lower = temp_word[0].lower() + temp_word[1:].lower() if len(temp_word) > 0 else ""
            else:
                temp_word_lower = temp_word.lower()


            for _ in range(num_errors_to_apply):
                if not temp_word_lower: break

                error_func = random.choice(error_functions)
                temp_word_lower = error_func(temp_word_lower)

            if original_first_char_case == 'upper' and temp_word_lower:
                 temp_word = temp_word_lower[0].upper() + temp_word_lower[1:]
            else:
                 temp_word = temp_word_lower

            misspelled_words.append(temp_word)
        else:
            misspelled_words.append(word)

    return " ".join(misspelled_words)

print("\nOriginal:", raw_tune_sentences[0])
for i in range(5):
    print(f"Misspelled {i+1}:", generate_misspelled_sentence_random_errors(raw_tune_sentences[0], error_rate_word=0.5))

MAX_SENTENCES_TUNE = None
MAX_SENTENCES_VAL_TEST = None
VERSIONS_PER_SENTENCE = 4

def create_paired_dataset(correct_sentences, num_versions=1, subset_name="train", max_sentences=None):
    misspelled_list = []
    correct_list = []

    if max_sentences:
        correct_sentences = correct_sentences[:max_sentences]

    print(f"Generating misspelled data for {subset_name} ({len(correct_sentences)} sentences, {num_versions} versions each)...")
    count = 0
    for sentence in correct_sentences:
        if not sentence.strip():
            continue
        for _ in range(num_versions):
            misspelled = generate_misspelled_sentence_random_errors(sentence)
            if misspelled.strip() and misspelled != sentence:
                misspelled_list.append(misspelled)
                correct_list.append(sentence)
        count += 1
        if count % (len(correct_sentences)//10 if len(correct_sentences) > 10 else 1) == 0:
             print(f"  Processed {count}/{len(correct_sentences)} original sentences for {subset_name}...")

    return pd.DataFrame({"misspelled": misspelled_list, "correct": correct_list})

train_df = create_paired_dataset(raw_tune_sentences, num_versions=VERSIONS_PER_SENTENCE, subset_name="train", max_sentences=MAX_SENTENCES_TUNE)
validation_df = create_paired_dataset(raw_validation_sentences, num_versions=1, subset_name="validation", max_sentences=MAX_SENTENCES_VAL_TEST)
test_df = create_paired_dataset(raw_test_sentences, num_versions=1, subset_name="test", max_sentences=MAX_SENTENCES_VAL_TEST)

print(f"\nGenerated {len(train_df)} training pairs.")
print(f"Generated {len(validation_df)} validation pairs.")
print(f"Generated {len(test_df)} test pairs.")

if not train_df.empty:
    print("\nSample of generated training data:")
    print(train_df.head())
else:
    print("Warning: Training DataFrame is empty. Check error generation or input data.")
    if MAX_SENTENCES_TUNE < 50 and VERSIONS_PER_SENTENCE ==1 :
        print("Consider increasing MAX_SENTENCES_TUNE or VERSIONS_PER_SENTENCE.")

MODEL_NAME = 't5-small'
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

PREFIX = "fix spelling: "
MAX_INPUT_LENGTH = 128
MAX_TARGET_LENGTH = 128

def preprocess_function(examples):
    inputs = [PREFIX + misspelled for misspelled in examples['misspelled']]
    targets = [correct for correct in examples['correct']]

    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding='max_length')

    labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding='max_length')

    model_inputs['labels'] = labels['input_ids']

    for i in range(len(model_inputs['labels'])):
        model_inputs['labels'][i] = [
            (l if l != tokenizer.pad_token_id else -100) for l in model_inputs['labels'][i]
        ]

    return model_inputs

if not train_df.empty and not validation_df.empty and not test_df.empty :
    dataset_train_hf = Dataset.from_pandas(train_df)
    dataset_val_hf = Dataset.from_pandas(validation_df)
    dataset_test_hf = Dataset.from_pandas(test_df)

    raw_datasets = DatasetDict({
        'train': dataset_train_hf,
        'validation': dataset_val_hf,
        'test': dataset_test_hf
    })

    print("\nTokenizing datasets...")
    tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=['misspelled', 'correct'])
    print(tokenized_datasets)

BATCH_SIZE = 32
def to_tf_dataset(dataset_hf, batch_size, shuffle=False):
    columns = ['input_ids', 'attention_mask', 'labels']
    dataset_hf.set_format(type='tensorflow', columns=columns)

    features = {x: dataset_hf[x] for x in ['input_ids', 'attention_mask']}
    labels = dataset_hf['labels']

    tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))
    if shuffle:
        tf_dataset = tf_dataset.shuffle(buffer_size=len(dataset_hf))
    tf_dataset = tf_dataset.batch(batch_size)
    return tf_dataset

tf_train_dataset = to_tf_dataset(tokenized_datasets['train'], BATCH_SIZE, shuffle=True)
tf_validation_dataset = to_tf_dataset(tokenized_datasets['validation'], BATCH_SIZE)
tf_test_dataset = to_tf_dataset(tokenized_datasets['test'], BATCH_SIZE)

print("\nSample from tokenized training data (first batch):")
for batch in tf_train_dataset.take(1):
    inputs, labels = batch
    print("Input IDs shape:", inputs['input_ids'].shape)
    print("Attention Mask shape:", inputs['attention_mask'].shape)
    print("Labels shape:", labels.shape)
    print("Decoded Input Sample:", tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False))
    print("Decoded Label Sample:", tokenizer.decode([l if l != -100 else tokenizer.pad_token_id for l in labels[0]], skip_special_tokens=False))
    break
else:
    print("Skipping tokenization and model training due to empty dataframes.")
    tf_train_dataset, tf_validation_dataset, tf_test_dataset = None, None, None

"""### T5 Model"""

from tf_keras.optimizers import AdamW as TfKerasAdamW


print("\nInitializing T5 model...")
model = TFT5ForConditionalGeneration.from_pretrained(MODEL_NAME)

LEARNING_RATE = 3e-5

print("Using optimizer from tf_keras.optimizers.AdamW")
optimizer_instance = TfKerasAdamW(learning_rate=LEARNING_RATE,weight_decay=0.01)
print(f"Optimizer instance created: {optimizer_instance}")

model.compile(optimizer=optimizer_instance)
print("Model compiled successfully with tf_keras.optimizers.AdamW!")

from tf_keras.callbacks import EarlyStopping

early_stopping_callback = EarlyStopping(
      monitor='val_loss',
      patience=2,
      restore_best_weights=True,
      verbose=1
)

NUM_EPOCHS = 3
print("\nStarting fine-tuning...")

history = model.fit(
        tf_train_dataset,
        validation_data=tf_validation_dataset,
        epochs=NUM_EPOCHS,
        callbacks=[early_stopping_callback]
        )

import numpy as np
import tensorflow as tf
from jiwer import wer, cer

def compute_metrics(model, dataset, tokenizer, prefix="fix spelling: "):
    total_batches = tf.data.experimental.cardinality(dataset).numpy()
    if total_batches < 0:
        total_batches = None

    all_refs = []
    all_hyps = []

    for batch_idx, batch in enumerate(dataset):
        features, labels = batch

        generated_ids = model.generate(
            input_ids=features["input_ids"],
            attention_mask=features["attention_mask"],
            max_length=tokenizer.model_max_length,
        )

        hyps = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        label_ids = np.where(labels.numpy() == -100,
                             tokenizer.pad_token_id,
                             labels.numpy())
        refs = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

        all_hyps.extend(hyps)
        all_refs.extend(refs)

        if total_batches:
            print(f"Batch {batch_idx+1}/{total_batches} completed")
        else:
            print(f"Batch {batch_idx+1} completed")

    overall_wer = wer(all_refs, all_hyps)
    overall_cer = cer(all_refs, all_hyps)
    return {
        "wer": overall_wer,
        "cer": overall_cer
        }

metrics = compute_metrics(model, tf_test_dataset, tokenizer)
print(f"\nFinal results → WER: {metrics['wer']:.3%}, CER: {metrics['cer']:.3%}, Exact-Match Accuracy: {metrics['exact_match_accuracy']:.3%}")

SAVE_DIRECTORY = "./my_spell_corrector_t5_small"
if not os.path.exists(SAVE_DIRECTORY):
    os.makedirs(SAVE_DIRECTORY)
    print(f"Created directory: {SAVE_DIRECTORY}")

print(f"\nSaving model to {SAVE_DIRECTORY}...")
model.save_pretrained(SAVE_DIRECTORY)
print("Model weights and config saved.")

print(f"Saving tokenizer to {SAVE_DIRECTORY}...")
tokenizer.save_pretrained(SAVE_DIRECTORY)
print("Tokenizer saved.")

def correct_sentences_in_batch(misspelled_sentences_list,model,tokenizer,prefix,max_input_length,max_target_length,num_beams=4,early_stopping=True):
    if not misspelled_sentences_list:
        return []

    prefixed_sentences = [prefix + sentence for sentence in misspelled_sentences_list]
    inputs = tokenizer(
        prefixed_sentences,
        return_tensors="tf",
        max_length=max_input_length,
        truncation=True,
        padding="longest"
    )
    summary_ids = model.generate(
        inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=max_target_length,
        num_beams=num_beams,
        early_stopping=early_stopping
    )
    corrected_sentences = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)
    return corrected_sentences


print("\n--- Example Inference with Function ---")

custom_sentences_misspelled = [
    "I have a qestion abot ths assignent",
    "teh qwik brwn fox jmps ovr teh lazy dog.",
    "he dont know nuthin abot programing.",
    "ths is anothr exmple to tst."
]

corrected_batch = correct_sentences_in_batch(custom_sentences_misspelled,model,tokenizer,PREFIX,MAX_INPUT_LENGTH,MAX_TARGET_LENGTH)
for original, corrected in zip(custom_sentences_misspelled, corrected_batch):
    print(f"Input Misspelled: {original}")
    print(f"Corrected Output: {corrected}")
    print("---")

sample_df = test_df.sample(5, random_state=42)
misspelled_examples = sample_df['misspelled'].tolist()
ground_truths       = sample_df['correct'].tolist()

predictions = correct_sentences_in_batch(
    misspelled_examples,
    model,
    tokenizer,
    PREFIX,
    MAX_INPUT_LENGTH,
    MAX_TARGET_LENGTH,
    num_beams=4,
    early_stopping=True
)

for i, (inp, pred, true) in enumerate(zip(misspelled_examples, predictions, ground_truths), 1):
    print(f"Example {i}")
    print(f" Misspelled: {inp}")
    print(f" Predicted : {pred}")
    print(f" Ground-truth: {true}")
    print("-" * 50)

"""### BART"""

from datasets import Dataset, DatasetDict
from transformers import BartTokenizerFast

MODEL_NAME = 'facebook/bart-base'
tokenizer = BartTokenizerFast.from_pretrained(MODEL_NAME)
MAX_INPUT_LENGTH = 128
MAX_TARGET_LENGTH = 128

def preprocess_function(examples):
    inputs = examples['misspelled']
    targets = [correct for correct in examples['correct']]
    model_inputs = tokenizer(inputs,
                             max_length=MAX_INPUT_LENGTH,
                             truncation=True,
                             padding='max_length')
    labels = tokenizer(text_target=targets,
                       max_length=MAX_TARGET_LENGTH,
                       truncation=True,
                       padding='max_length')
    model_inputs['labels'] = labels['input_ids']
    for i in range(len(model_inputs['labels'])):
        model_inputs['labels'][i] = [
            (label_id if label_id != tokenizer.pad_token_id else -100)
            for label_id in model_inputs['labels'][i]
        ]
    return model_inputs

dataset_train_hf = Dataset.from_pandas(train_df)
dataset_val_hf = Dataset.from_pandas(validation_df)
dataset_test_hf = Dataset.from_pandas(test_df)
raw_datasets = DatasetDict({
        'train': dataset_train_hf,
        'validation': dataset_val_hf,
        'test': dataset_test_hf
    })

tokenized_datasets = raw_datasets.map(
        preprocess_function,
        batched=True,
        remove_columns=['misspelled', 'correct']
    )
print(tokenized_datasets)
print("\nExample of tokenized train data (first item):")
example = tokenized_datasets['train'][0]
print("Input IDs:", example['input_ids'])
print("Decoded Input:", tokenizer.decode(example['input_ids'], skip_special_tokens=False))
print("Labels:", example['labels'])

decoded_labels_ids = [l if l != -100 else tokenizer.pad_token_id for l in example['labels']]
print("Decoded Labels:", tokenizer.decode(decoded_labels_ids, skip_special_tokens=False))
print("Attention Mask:", example['attention_mask'])

BATCH_SIZE = 32
def to_tf_dataset(dataset_hf, batch_size, shuffle=False):
        columns = ['input_ids', 'attention_mask', 'labels']
        dataset_hf.set_format(type='tensorflow', columns=columns)

        features = {x: dataset_hf[x] for x in ['input_ids', 'attention_mask']}
        labels = dataset_hf['labels']

        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))
        if shuffle:
            tf_dataset = tf_dataset.shuffle(buffer_size=len(dataset_hf))
        tf_dataset = tf_dataset.batch(batch_size)
        return tf_dataset

tf_train_dataset = to_tf_dataset(tokenized_datasets['train'], BATCH_SIZE, shuffle=True)
tf_validation_dataset = to_tf_dataset(tokenized_datasets['validation'], BATCH_SIZE)
tf_test_dataset = to_tf_dataset(tokenized_datasets['test'], BATCH_SIZE)

for batch in tf_train_dataset.take(1):
        inputs, labels = batch
        print("Input IDs shape:", inputs['input_ids'].shape)
        print("Attention Mask shape:", inputs['attention_mask'].shape)
        print("Labels shape:", labels.shape)
        print("Decoded Input Sample:", tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False))
        print("Decoded Label Sample:", tokenizer.decode([l if l != -100 else tokenizer.pad_token_id for l in labels[0]], skip_special_tokens=False))
        break

from transformers import TFBartForConditionalGeneration
from tf_keras.optimizers import AdamW as TfKerasAdamW
from tf_keras.optimizers import AdamW as KerasAdamW

model = TFBartForConditionalGeneration.from_pretrained(MODEL_NAME)
LEARNING_RATE = 3e-5
WEIGHT_DECAY = 0.01
optimizer_instance = KerasAdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
print(f"Optimizer instance created: {optimizer_instance}")
model.compile(optimizer=optimizer_instance)

NUM_EPOCHS = 2
history = model.fit(
        tf_train_dataset,
        validation_data=tf_validation_dataset,
        epochs=NUM_EPOCHS
        )

def compute_metrics(model, dataset: tf.data.Dataset, tokenizer):
    all_refs = []
    all_hyps = []
    iiii=0
    for batch in dataset:
        features, labels = batch
        generated_ids = model.generate(
            input_ids=features["input_ids"],
            attention_mask=features["attention_mask"],
            max_length=MAX_TARGET_LENGTH,
            num_beams=4,
            early_stopping=True
        )
        print(f"batch: {iiii}")
        iiii+=1
        hyps = tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)
        label_ids_np = labels.numpy()
        label_ids_for_decode = np.where(label_ids_np == -100, tokenizer.pad_token_id, label_ids_np)
        refs = tokenizer.batch_decode(label_ids_for_decode, skip_special_tokens=True)
        all_hyps.extend(hyps)
        all_refs.extend(refs)
        if iiii == 10:
          break
    overall_wer = wer(all_refs, all_hyps)
    overall_cer = cer(all_refs, all_hyps)
    return {
        "wer": overall_wer,
        "cer": overall_cer
        }

metrics = compute_metrics(model, tf_test_dataset, tokenizer)
print(f"\nTest Results → WER: {metrics['wer']:.3%}, CER: {metrics['cer']:.3%}")

def predict_spelling_single_string(model, tokenizer, raw_text, prefix, max_input_length, max_target_length):
    tokenized_input = tokenizer(
        [raw_text],
        max_length=max_input_length,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )
    generated_ids = model.generate(
        input_ids=tokenized_input["input_ids"],
        attention_mask=tokenized_input["attention_mask"],
        max_length=max_target_length,
        num_beams=4,
        early_stopping=True
    )
    prediction = tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)[0]
    return prediction

custom_sentences = [
    "Whre are you giong",
    "Teh meat will b tasty",
    "Harry where re you living"
]
print("--- Testing with Custom Sentences ---")
for sentence in custom_sentences:
    prediction = predict_spelling_single_string(
        model, tokenizer, sentence, PREFIX, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
    )
    print(f"Input:     {sentence}")
    print(f"Predicted: {prediction}")
    print("-" * 30)

def predict_spelling_single_string(model, tokenizer, raw_text, prefix, max_input_length, max_target_length):
    input_text = raw_text
    tokenized_input = tokenizer(
        [input_text],
        max_length=max_input_length,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )
    generated_ids = model.generate(
        input_ids=tokenized_input["input_ids"],
        attention_mask=tokenized_input["attention_mask"],
        max_length=max_target_length,
        num_beams=4,
        early_stopping=True
    )
    prediction = tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)[0]
    return prediction

indices_to_test = [0, 1, 5, 65, 50]
for i in indices_to_test:
    misspelled_input = test_df.loc[i, 'misspelled']
    correct_reference = test_df.loc[i, 'correct']
    prediction = predict_spelling_single_string(
        model, tokenizer, misspelled_input, PREFIX, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
    )
    print(f"Index {i}:")
    print(f"Input (Misspelled): {misspelled_input}")
    print(f"Reference (Correct):{correct_reference}")
    print(f"Predicted:          {prediction}")
    print("-" * 40)

"""### Fine Tuned BART"""

MODEL_CHECKPOINT = "oliverguhr/spelling-correction-english-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
MAX_INPUT_LENGTH = 128
MAX_TARGET_LENGTH = 128

def preprocess_function(examples):
    inputs = examples['misspelled']
    targets = [correct for correct in examples['correct']]
    model_inputs = tokenizer(inputs,
                             max_length=MAX_INPUT_LENGTH,
                             truncation=True,
                             padding='max_length')
    labels = tokenizer(text_target=targets,
                       max_length=MAX_TARGET_LENGTH,
                       truncation=True,
                       padding='max_length')
    model_inputs['labels'] = labels['input_ids']
    for i in range(len(model_inputs['labels'])):
        model_inputs['labels'][i] = [
            (label_id if label_id != tokenizer.pad_token_id else -100)
            for label_id in model_inputs['labels'][i]
        ]
    return model_inputs


dataset_train_hf = Dataset.from_pandas(train_df)
dataset_val_hf = Dataset.from_pandas(validation_df)
dataset_test_hf = Dataset.from_pandas(test_df)
raw_datasets = DatasetDict({
        'train': dataset_train_hf,
        'validation': dataset_val_hf,
        'test': dataset_test_hf
    })

tokenized_datasets = raw_datasets.map(
        preprocess_function,
        batched=True,
        remove_columns=['misspelled', 'correct']
    )
print(tokenized_datasets)
print("\nExample of tokenized train data (first item):")
example = tokenized_datasets['train'][0]
print("Input IDs:", example['input_ids'])
print("Decoded Input:", tokenizer.decode(example['input_ids'], skip_special_tokens=False))
print("Labels:", example['labels'])

decoded_labels_ids = [l if l != -100 else tokenizer.pad_token_id for l in example['labels']]
print("Decoded Labels:", tokenizer.decode(decoded_labels_ids, skip_special_tokens=False))
print("Attention Mask:", example['attention_mask'])

MODEL_CHECKPOINT

model = TFBartForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)
LEARNING_RATE = 3e-5
WEIGHT_DECAY = 0.01
optimizer_instance = KerasAdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
print(f"Optimizer instance created: {optimizer_instance}")
model.compile(optimizer=optimizer_instance)

NUM_EPOCHS = 2
history = model.fit(
        tf_train_dataset,
        validation_data=tf_validation_dataset,
        epochs=NUM_EPOCHS
        )

def compute_metrics(model, dataset: tf.data.Dataset, tokenizer):
    all_refs = []
    all_hyps = []
    iiii=0
    for batch in dataset:
        features, labels = batch
        generated_ids = model.generate(
            input_ids=features["input_ids"],
            attention_mask=features["attention_mask"],
            max_length=MAX_TARGET_LENGTH,
            num_beams=4,
            early_stopping=True
        )
        print(f"batch: {iiii}")
        iiii+=1
        hyps = tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)
        label_ids_np = labels.numpy()
        label_ids_for_decode = np.where(label_ids_np == -100, tokenizer.pad_token_id, label_ids_np)
        refs = tokenizer.batch_decode(label_ids_for_decode, skip_special_tokens=True)
        all_hyps.extend(hyps)
        all_refs.extend(refs)
        if iiii == 10:
          break
    overall_wer = wer(all_refs, all_hyps)
    overall_cer = cer(all_refs, all_hyps)

    return {
        "wer": overall_wer,
        "cer": overall_cer
        }

metrics = compute_metrics(model, tf_test_dataset, tokenizer)
print(f"\nTest Results → WER: {metrics['wer']:.3%}, CER: {metrics['cer']:.3%}")

def predict_spelling_single_string(model, tokenizer, raw_text, prefix, max_input_length, max_target_length):
    tokenized_input = tokenizer(
        [raw_text],
        max_length=max_input_length,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )
    generated_ids = model.generate(
        input_ids=tokenized_input["input_ids"],
        attention_mask=tokenized_input["attention_mask"],
        max_length=max_target_length,
        num_beams=4,
        early_stopping=True
    )
    prediction = tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)[0]
    return prediction

custom_sentences = [
    "Whre are you giong",
    "Teh meat will b tasty",
    "Harry where re yu living",
    "Iam runing int the cra",
    "my naem is Ziyad ",
    "hs name is essam"
]

print("--- Testing with Custom Sentences ---")
for sentence in custom_sentences:
    prediction = predict_spelling_single_string(
        model, tokenizer, sentence, PREFIX, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
    )
    print(f"Input:     {sentence}")
    print(f"Predicted: {prediction}")
    print("-" * 30)

def predict_spelling_single_string(model, tokenizer, raw_text, prefix, max_input_length, max_target_length):
    input_text = raw_text
    tokenized_input = tokenizer(
        [input_text],
        max_length=max_input_length,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )

    generated_ids = model.generate(
        input_ids=tokenized_input["input_ids"],
        attention_mask=tokenized_input["attention_mask"],
        max_length=max_target_length,
        num_beams=4,
        early_stopping=True
    )
    prediction = tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)[0]
    return prediction


indices_to_test = [0, 1, 5, 65, 50]
for i in indices_to_test:
    misspelled_input = test_df.loc[i, 'misspelled']
    correct_reference = test_df.loc[i, 'correct']
    prediction = predict_spelling_single_string(
        model, tokenizer, misspelled_input, PREFIX, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
    )
    print(f"Index {i}:")
    print(f"Input (Misspelled): {misspelled_input}")
    print(f"Reference (Correct):{correct_reference}")
    print(f"Predicted:          {prediction}")
    print("-" * 40)